#! /usr/bin/env python2

import os
import sys
import json
import yaml
#from buginfo import BugInfo
from CredentialStore import CredentialStore
from launchpadlib.launchpad import Launchpad
from launchpadlib.credentials import Credentials
from lazr.restfulclient.errors import HTTPError
  
def main():
    (project, title, json_loc, arsenal_scripts, arsenal_templates) = ('oil', 
    'OIL', '/home/darren/Repositories/Canonical/' + 
    'arsenal/arsenal-devel/reports/OIL/OIL.json', 
    '/home/darren/Repositories/Canonical/webapp/trunk/arsenal-devel/scripts', 
    '/home/darren/Repositories/Canonical/webapp/trunk/arsenal-devel/web/templates')
                  
    WebAppBuilder(project, title, json_loc, arsenal_scripts, 
               arsenal_templates)

    
class WebAppBuilder(object):
    
    def __init__(self, project, title, json_loc, arsenal_scripts, 
               arsenal_templates):        
        #buginfo = self.get_launchpad_creds(json_loc, project)
        self.gen_webapp(title, json_loc, arsenal_scripts, arsenal_templates)

    def gen_webapp(self, title, json_loc, arsenal_scripts, 
                   arsenal_templates): 
        """
            Uses doberman.reformer to collate all of the outputs from the 
            different refinery jobs and stick them in a single output location and 
            recheck the error against the bugs database. 
            Then uses the output from run_arsenal to display the following:
                - Make a new filter for jobs and list them by number of hits
                - Have a date range or some choices (i.e. so you could have today, 
                    yesterday, last week, last month, last year, all time, etc) in 
                    the options
                - So we'd have a list of bugs ordered by number encountered in the 
                    given time period, separated into 'pipeline_deploy', 
                    'pipeline_prepare', and 'test_tempest_smoke'. One for PS and 
                    one for staging.
                - Show what we are currently testing and whether a job failed or 
                    passed. In other words, be able to just show, say, VNX with a 
                    list of pipelines that succeeded.
                - Include charts at the bottom (generated by running charter on 
                    the output of reformer, as this should be exactly the same 
                    format as the output of refinery).
                - It would be static and regenerated in the same way as the arsenal 
                    pages are now, so once I get around to sorting that cron job 
                    out they can both do the same.
            
            
            Using the same LP OIL.json/bugs, I need to do one for staging and one 
            for PS. Run this cron job on there and copy to the MAAS server then.
        """
        
        self.combine_harvester()  # <- load up existing json file and pass in here as data


    def combine_harvester(self, data={}):
        
        json_loc = '/home/darren/Repositories/Canonical/webapp/trunk/mock_data/OIL.json'
        stats_loc = '/home/darren/Repositories/Canonical/webapp/trunk/mock_data/oil-stats.json'
        refinery_output_dir = '/home/darren/Repositories/Canonical/webapp/trunk/mock_data/reformer_output_daily'
        
        with open(json_loc) as jfile: 
            lp_data = json.load(jfile).get('tasks')
        
        with open(stats_loc) as sfile: 
            stats = json.load(sfile)
        
        bug_ranking_files = [output_file for output_file in 
                             os.listdir(refinery_output_dir) if 'bug_ranking' 
                             in output_file and 'all' not in output_file]
        for bug_ranking in bug_ranking_files:
            with open(os.path.join(refinery_output_dir, bug_ranking)) as bfile: 
                key = (bug_ranking.split('.')[0].replace('bug_ranking', '')
                       .strip('_'))

                pipeline_ranking_data = yaml.load(bfile).get('pipelines')
                for pipeline_ranking_datum in pipeline_ranking_data:
                    bug_number = pipeline_ranking_datum[0]
                    number = int(pipeline_ranking_datum[1])
                    
                    # Bug number:
                    if bug_number not in data:
                        data[bug_number] = {}    
                        
                    # Get data from Launchpad:  
                    bug_data = lp_data.get(bug_number)
                    if bug_data:
                        data = self.extract_data_from_lp(bug_data, data)   
                        
                    # Get data from Launchpad:  
                    output = ''
                    if output:
                        data = self.extract_data_from_doberman_output(bug_data,
                                                                      data)
                    import pdb; pdb.set_trace()
                    
                    '''
                        
                    if 'affected pipelines' not in data[bug_number]:
                        data[bug_number]['affected pipelines'] = {}
                        
                    if pipeline not in data[bug_number]['affected pipelines']:
                    '''    
                        
                    # import pdb; pdb.set_trace()
                                        
                    '''
                    if pipeline not in bug_rankings[key]:
                        bug_rankings[key][pipeline] = number
                    else:
                        import pdb; pdb.set_trace()
                        # add together?
                        # TODO but only if the data hasn't been seen before - how'd I do that then?
                    '''
        import pdb; pdb.set_trace()
        #write out
        
        # json_loc (OIL.json) and the output of reformer (which will be scp'd
        # over from a jenkins job) have everything we need, so we DO NOT need 
        # to write a new reporter, only a new mako template (althoughb feel 
        # free to add to the existing reporter).
        
        '''
            webapp to import in data

                
            Then keep only this info in a json file and leaving the rest to be 
            overwritten...
        '''

    def extract_data_from_lp(self, bug_data, data):
        """ """
        
        bug_data = bug_data[0]
        
        title = bug_data.get('title')
        summary = title.split('"')[1]
        bug_num = title.split('"')[0].split('#')[1].split(' ')[0]
        
        data[bug_number]['tags'] = bug_data['bug'].get('tags')
        data[bug_number]['summary'] = summary
        data[bug_number]['bug_number'] = bug_num
        data[bug_number]['importance'] = bug_data.get('importance')
        data[bug_number]['status'] = bug_data.get('status')
        data[bug_number]['assignee'] = bug_data.get('assignee')
        data[bug_number]['owner'] = bug_data.get('owner')
        data[bug_number]['created'] = \
            bug_data['bug'].get('date_created')
        data[bug_number]['web_link'] = bug_data.get('web_link') 
        
        return data      
        
    def extract_data_from_doberman_output(self, output, data):
        import pdb; pdb.set_trace()
        # time 
        #data[bug_number][''] = bug_data.get('')
        
        # Get data from Doberman output: 
        
        # job(s)
        
        # states
        
        # machines
        
        # units
        
        # vendors
        
        # charms
        
        # slaves
        
        # ports
        
        # xunit name/class (optional?)
        
        # target file and/or regexp
        
        # bootstrap_node jenkins (e.g. 'Building on master'; optional?)
        
        # openstack release                         
        
        # affected pipelines
        
        return data
            
    
if __name__ == "__main__":
    sys.exit(main())
    

    
